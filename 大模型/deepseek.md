目录
DeepSeek-R1-Zero
DeepSeek-R1
蒸馏


学习方式：
1.重复和模仿-对应机器学习中的监督学习
2.trail error  尝试然后失败 换另外一种方式去尝试 -对应强化学习

deep sk v3 zero
V3的这个啊base model基座模型  它不断的去做这个纯粹的reinforcement learning

自然涌现的数学推理能力

zero的问题是推理能力不够强大和 双语回答

r1 使用了sft  使用优质的数据集 带人工标注的后训练 产生了checkpoint
依托于这个checkpoint 再进行一次类似zero的纯粹的强化学习

蒸馏：使用Qwen2.5和llama3基座模型 
qwen 32b +SFT =deepseek qwen 32b 将能力差的基座模型 显著提升

专注于大模型意义不大
重要的是AI应用 如何交互？
如果直接生成 前端页面 不好修改
生成中间层 设计软件类似Figma支持的格式
再进一步生成前端项目 使得容易修改
这一部分再post train部分可以优化 pre train的基座模型没必要花时间



以前的工作严重依赖于大量的监督数据来提高模型的性能。在本研究中，我们证明了即使不使用监督微调（SFT）作为冷启动，也可以通过大规模强化学习（RL）显著提高推理能力。此外，还可以通过包含少量冷启动数据来进一步增强性能。在以下章节中，我们介绍：(1)DeepSeek-R1- zero，它直接将RL应用于基础模型，而不需要任何SFT数据；(2)DeepSeek-R1，它从检查点开始应用RL，其中包含数千个长思维链（CoT）示例。3)将DeepSeek-R1的推理能力提炼为小的密集模型


强化学习在推理任务中已经证明了显著的有效性(Shao et al., 2024；Wang et al., 2023)。然而，这些工作在很大程度上依赖于监督数据，这需要大量的时间来收集。在本节中，我们将探讨llm在没有任何监督数据的情况下开发推理能力的潜力，重点关注它们通过纯强化学习过程的自我进化



在不需要任何监督微调数据的情况下获得强大的推理能力。这是一个值得注意的成就，因为它强调了模型仅通过强化学习有效地学习和泛化的能力

通过直接从基本模型开始强化学习，我们可以密切监控模型的进展，而不受监督微调阶段的影响。这种方法提供了模型如何随时间发展的清晰视图，特别是在处理复杂推理任务的能力方面

在DeepSeek-R1-Zero的训练过程中观察到的一个特别有趣的现象是“顿悟时刻”的出现。这个时刻，如表3所示，发生在模型的中间版本中。在这个阶段，DeepSeek-R1-Zero通过重新评估其初始方法来学习分配更多的思考时间给问题。这种行为不仅证明了模型不断增长的推理能力，也是强化学习如何导致意想不到的复杂结果的一个迷人例子。这一刻不仅是模型的“顿悟时刻”，也是观察其行为的研究人员的“顿悟时刻”。它强调了强化学习的力量和美妙之处：我们不是明确地教模型如何解决问题，而是简单地为它提供正确的激励，它就会自主地开发出先进的解决问题的策略



**蒸馏**
几个billion的模型 