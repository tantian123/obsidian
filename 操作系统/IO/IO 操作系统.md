[(108条消息) 详解磁盘IO、网络IO、零拷贝IO、BIO、NIO、AIO、IO多路复用(select、poll、epoll)_雷恩Layne的博客-CSDN博客_网络io和磁盘io的区别](https://blog.csdn.net/qq_37555071/article/details/113932533)

[(20 封私信 / 80 条消息) io - 搜索结果 - 知乎 (zhihu.com)](https://www.zhihu.com/search?type=content&q=io)

IO就是输入和输出
针对不同的对象，IO可以分为磁盘IO和网络IO
![[Pasted image 20221115224813.png]]
![[Pasted image 20221115224820.png]]





**内存空间分为用户空间和内核空间**（也可以称作用户缓冲区和内核缓冲区）
操作系统的应用不能直接操作内核空间，需要先将数据拷贝到用户空间
**read和write操作**，都只能在内核空间执行。

虚拟内存分为用户空间和内核空间，是逻辑上的划分

操作系统会给每个进程分配独立的虚拟内存地址空间，高地址的内存空间给系统内核使用，其他给用户进程使用。

进程进行**系统调用**（系统调用其实就是函数调用，只不过调用的是内核态的函数），陷入内核代码的时候，就是内核态
进程运行用户代码的时候，就是用户态

用户态和内核态的切换方式， 系统调用是进程主动，异常和中断都是被动
1.系统调用 比如上面示例中的写文件调用
2.异常 cpu在用户空间执行代码的时候，出现了异常，就会切换到处理异常的内核相关程序中
3.中断 外围设备完成用户请求的某些操作后，会向CPU发送相应的中断信号。比如硬盘数据读写完成




read操作和write操作
读操作：操作系统检查内核缓冲区有没有需要的数据，
	如果没有，且是磁盘IO，就从磁盘读取到内核缓冲区；
	如果是网络IO，就会等待客户端发送数据，这个过程中应用程序会阻塞，直到客户端发送了数据，该应用程序会唤醒，从socket协议栈中读取客户端发送的数据到内核空间，然后把数据拷贝到用户空间。
写操作：用户的应用程序将数据从用户空间拷贝到内核空间，这时候写操作就完成了。
至于是写入磁盘或者是网络传输，由操作系统决定





IO 设备和内存之间的数据传输方式  ：PIO DMA（直接内存访问）
从磁盘加载数据到内核的方式，PIO模式的计算机我们现在已经很少见到了

区别 ：从磁盘缓冲区到内存缓冲区的时候，是否占用CPU

![[Pasted image 20221113221709.png]]

DMA就是CPU的一个代理，它负责了一部分的拷贝工作，从而减轻了CPU的负担。




## **缓冲IO和直接IO**

用户空间是不能直接访问内核空间的数据的，如果需要访问怎么办？很简单，就需要将数据从内核空间**拷贝**的用户空间。
-   缓冲 IO：其实就是磁盘中的数据通过 `DMA` 先拷贝到内核空间，然后再从内核空间拷贝到用户空间。
-   直接 IO：磁盘中的数据直接通过 `DMA` 拷贝到用户空间。

缓冲 IO 也被称为标准 IO，在Linux的缓冲I/O机制中，数据先从磁盘复制到内核空间的缓冲区，然后从内核空间缓冲区复制到应用程序的地址空间。

优点：安全
缺点：不能直接在用户地址空间和磁盘之间进行数据传输，这样数据在传输过程中需要在**应用程序地址空间（用户空间）和内核缓冲（内核空间）之间进行多次数据拷贝操作**，开销很大

直接`IO`就是应用程序直接访问磁盘数据，而不经过内核缓冲区，也就是绕过内核缓冲区,自己管理`I/O`缓冲区，这样做的目的是减少一次从内核缓冲区到用户程序缓冲的数据复制。
优点：**减少一次从内核缓冲区到用户程序缓冲的数据复制**
缺点：直接加载会非常缓慢 ，通常 `直接I/O` 跟 `异步I/O` 结合使用会得到较好的性能。（异步IO：当访问数据的线程发出请求之后，线程会接着去处理其他事，而不是阻塞等待）





## 磁盘IO和网络IO


## 零拷贝 IO
在上述IO中，一次读写操作要经过**四次缓冲区的拷贝**，并经历了**四次内核态和用户态的切换**。 **零拷贝（zero copy）IO** 技术减少不必要的内核缓冲区跟用户缓冲区之间的拷贝，从而减少CPU的开销和状态切换带来的开销，达到性能的提升。




网络IO
读操作
**网络IO的既可以从物理磁盘中读数据，也可以从socket中读数据（从网卡中获取）。当从物理磁盘中读数据的时候，其流程和磁盘IO的读操作一样。当从socket中读数据，应用程序需要等待客户端发送数据，如果客户端还没有发送数据，对应的应用程序将会被阻塞，直到客户端发送了数据，该应用程序才会被唤醒，从Socket协议找中读取客户端发送的数据到内核空间（这个过程也由DMA控制），然后把内核空间的数据copy到用户空间，供应用程序使用。**
写操作

为了简化描述，我们假设网络IO的数据从磁盘中获取，读写操作的流程如下：

当应用程序调用read()方法时，通过DMA方式将数据从磁盘拷贝到内核缓冲区
由cpu控制，将内核缓冲区的数据拷贝到用户空间的缓冲区中，供应用程序使用
当应用程序调用write()方法时，cpu会把用户缓冲区中的数据copy到内核缓冲区的Socket Buffer中
最后通过DMA方式将内核空间中的Socket Buffer拷贝到Socket协议栈（即网卡设备）中传输。
网络IO的写操作也有四次缓冲区的copy，第一次是从磁盘缓冲区到内核缓冲区（由cpu控制），第二次是内核缓冲区到用户缓冲区（DMA控制），第三次是用户缓冲区到内核缓冲区的Socket Buffer（由cpu控制），第四次是从内核缓冲区的Socket Buffer到网卡设备（由DMA控制）。四次缓冲区的copy工作两次由cpu控制，两次由DMA控制。


简单来说，就是
数据从网卡，通过DMA进入内存 ringBuffer区域
然后进入socket



socket的所有操作都是操作系统提供的，所以都要通过系统调用来完成





IO模型
为什么有？
同步阻塞 同步非阻塞 IO多路复用

同步阻塞式IO 
调用read方法时，会阻塞在那

每处理一个socket占用一个线程 高并发场景下会加剧线程调度的开销

非阻塞IO 
调用read方法时，不会阻塞在那，立即返回结果

不让出CPU，但是会频繁检查socket是否就绪，这也是一种消耗

IO多路复用
目的：一个线程 同时处理多个socket

操作系统将需要等待的socket加入监听集合
，这样就可以通过一次系统调用，同时监听多个socket


进程通过fd文件描述符知道是否就绪，一个fd关联linux中的一个文件，当然包括套接字，一切皆文件

linux提供了三种实现IO多路复用的方式（IO多路复用是指 使用单个线程监听多个fd，并在就绪时通知）
1.select
fd数组是unsigned long型的数组，16个元素，每一位对应一个fd，所以是16* 64=1024个
太少了
**遍历一遍，返回就绪的fd，
针对这些fd，去调用read方法**

就不会阻塞了（因为我通过遍历去查询过它的状态了）
2.poll
fd链表 支持的最大连接数 是文件描述符的个数
3.epoll

三个方法：
epollcreate 创建epoll 获得句柄
epolletl 添加或者删除fd
按需要创建数据结构，存储对应的fd
**epoll wait 获取到就绪的fd 不需要遍历（存在一个队列中）**

通过io多路复用 线程不需要等待某个socket 而阻塞或者空耗CPU

但是问题在于
可读的socket可能是不完整的，需要下一次可读，这时候需要记录socket状态，保存上下文，切换到下一个就绪的fd,
下次该socket可读时，需要恢复保存的现场从而继续处理。
我们需要随着事件的等待和就绪，频繁地保存和恢复现场
这时，适合使用协程

## Java网络IO模型
## BIO
对应Linux内核的同步阻塞IO模型







