1线性回归
线性模型 
损失函数（将学习问题 转为 优化问题） 
链式法则（vector下的）
梯度下降（优化损失函数）

2决策树
树模型
boost提升算法
GBDT 梯度提升决策树 (是一个baseline的模型)




## 线性模型
![[Pasted image 20250218231625.png]]


例子：公司股价预测，通过公司利润 员工数 负债率 esg评分 等等（特征工程 给模型寻找特征 然后去寻找数据）

对每个因子寻找一个系数 


通过损失函数cost function 将学习问题转为优化问题
### **常见的损失函数**

1. **均方误差（Mean Squared Error，MSE）**：
![[Pasted image 20250218232529.png]]

![[Pasted image 20250218232704.png]]


但是学习问题本质并不是优化问题
所以存在过拟合 泛化能力差

关键是避免过拟合问题



优化问题
凸优化：解法   梯度优化和对偶性转换

梯度下降
梯度：y在不同x下的偏导数组成的向量

即沿着梯度的方向 下降最快  即最优方向 最快达到全局最优点
等于0的时候就是极值点


凸函数是指线段上方是凹还是凸
![[Pasted image 20250218233700.png]]


凸函数才有全局最优点
这个就不是凸函数 是非凸问题  是局部最优解 此时需要更大的补偿 跳过局部最优
![[Pasted image 20250218233803.png]]


如何求梯度 对每个变量求偏导：
链式法则 chain rule
![[Pasted image 20250218234114.png]]


链式法则在向量上的推广
[[PDF] The Matrix Calculus You Need For Deep Learning | Semantic Scholar](https://www.semanticscholar.org/reader/392ca3a7132c9ab6b3c88088f6d861c9fad25f40)



总结
线性模型 
损失函数-均方误差 学习问题转为优化问题
用梯度下降 解凸函数优化，核心是求梯度（初始梯度计算误差 计算梯度 不断循环直到全局最小即斜率为0）
数学算法：向量化的链式法则

